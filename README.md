To run our script that pulls data from HuggingFace and curates it for our React code generation task, first import the following libraries:

> pip install --upgrade google-api-python-client

Then you may run main.py, which outputs a CSV file with the curated data:

> python3 main.py

Once the data is extracted from main.py, you can feed it into the DataIngestion Python notebook to generate the four datasets used for fine-tuning: the Full (Combined) training set of 1500 examples (750 React curated, 750 OSS-Instruct), the curated React dataset of 1500 examples, the adapted OSS-Instruct dataset of 1500 examples, and the test set of 50 examples. 

As long as you put the dataset CSV files generated by the DataIngestion notebook under a ./data directory, you should be able to run the fine-tuning using the CodeLlamaFineTuning and PredibaseFineTuning Python notebooks. 

The CodeLlamaFineTuning notebook uses HuggingFace's Trainer module to fine-tune the CodeLlama-Instruct-7B model. The PredibaseFineTuning Python notebook starts a Predibase job to fine-tune the CodeLlama-Instruct-13B model, which you may view from the link that is output by the notebook. 

The evaluation functions are embedded within each notebook to run evaluation on the test set and HumanEval benchmark both before and after fine-tuning completes.